{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "# 显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# 显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# 设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth', 100)\n",
    "file = '../../dataset/ibtracs.WP.list.v04r00.csv'\n",
    "\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "# data = data[['SID', 'BASIN', 'SUBBASIN', 'ISO_TIME','NATURE', 'LAT', 'LON', 'WMO_WIND', 'WMO_PRES', 'WMO_AGENCY', 'DIST2LAND', 'LANDFALL',\n",
    "#               'USA_AGENCY','USA_R34_NE','USA_R34_SE',\"USA_R34_SW\",'USA_R34_NW','USA_R50_NE','USA_R50_SE','USA_R50_SW','USA_R50_NW',\n",
    "#              'USA_POCI','USA_ROCI','USA_RMW','USA_EYE','TOKYO_R50_DIR','TOKYO_R50_LONG','TOKYO_R50_SHORT','TOKYO_R30_DIR',\n",
    "#              'TOKYO_R30_LONG','TOKYO_R30_SHORT','TOKYO_LAND','STORM_SPEED','STORM_DIR'\n",
    "# ]]\n",
    "# data = data[['SID', 'ISO_TIME','LON', 'LAT','STORM_SPEED', 'DIST2LAND', 'TRACK_TYPE', \n",
    "#        'STORM_DIR',   'NATURE', 'SUBBASIN',\n",
    "#        'BASIN', 'LANDFALL', 'USA_SSHS', 'USA_ATCF_ID','USA_WIND', 'TOKYO_R30_DIR', 'TOKYO_GRADE', 'TOKYO_R50_DIR',\n",
    "#        'TOKYO_PRES',  'USA_STATUS', 'CMA_CAT',\n",
    "#        'USA_PRES',  'CMA_PRES', 'CMA_WIND', 'USA_POCI',\n",
    "#        'USA_ROCI', 'USA_RMW', 'HKO_CAT', 'HKO_WIND', 'HKO_PRES',\n",
    "#         'TOKYO_R30_LONG', 'TOKYO_WIND', 'TOKYO_R30_SHORT',\n",
    "#        'USA_R34_NE', 'USA_R34_SE', 'USA_R34_NW', 'USA_R34_SW', 'TOKYO_LAND',\n",
    "#        'WMO_AGENCY', 'WMO_PRES', 'USA_AGENCY', 'WMO_WIND', 'TOKYO_R50_LONG',\n",
    "#        'TOKYO_R50_SHORT', 'USA_R50_NE', 'USA_R50_SE', 'USA_R50_NW',\n",
    "#        'USA_R50_SW']]\n",
    "data = data[['SID', 'ISO_TIME','LON', 'LAT','STORM_SPEED', 'DIST2LAND', 'TRACK_TYPE', 'STORM_DIR', 'NATURE', 'SUBBASIN','BASIN', 'LANDFALL', 'USA_WIND', 'TOKYO_R30_DIR', 'TOKYO_GRADE', 'TOKYO_R50_DIR','TOKYO_PRES', 'USA_POCI','USA_ROCI', 'USA_RMW', 'TOKYO_R30_LONG', \n",
    "             'TOKYO_R30_SHORT', 'USA_R34_NE', 'USA_R34_SE', 'USA_R34_NW', 'USA_R34_SW', 'TOKYO_LAND',  'TOKYO_R50_LONG', 'TOKYO_R50_SHORT', 'USA_R50_NE', 'USA_R50_SE', 'USA_R50_NW','USA_R50_SW']]\n",
    "data = data[1:]\n",
    "data['ISO_TIME'] =  pd.to_datetime(data['ISO_TIME'])\n",
    "begin = datetime.datetime(2000,1,1,0,0,0)\n",
    "# 选择西北太平洋 且2000年之后的数据\n",
    "data = data[(data['BASIN']=='WP') & (data['ISO_TIME']>=begin)]\n",
    "dataNew = data.replace({' ':np.nan})\n",
    "dataNew.to_csv('original_data.csv')\n",
    "# print(dataNew.shape)\n",
    "missing = dataNew.isnull().sum()\n",
    "# print(missing)\n",
    "\n",
    "#缺失值的展示 \n",
    "# def plot_missing(df):\n",
    "#     # 对缺失值列进行计数\n",
    "#     missing = df.isnull().sum()\n",
    "#     missing = missing[missing > 0]\n",
    "#     #missing.sort_values(inplace=True)\n",
    "#     missing.plot.bar(figsize=(12,6))\n",
    "#     missing.sort_values(inplace=True)\n",
    "# #     msno.matrix(df,)\n",
    "#     plt.savefig('missing.png')\n",
    "# plot_missing(dataNew)\n",
    "# print('-------------------')\n",
    "\n",
    "begin = datetime.datetime(2019,1,1,0,0,0)\n",
    "end = datetime.datetime(2020,1,1,0,0,0)\n",
    "year20 = dataNew[(data['ISO_TIME']>=begin) & (data['ISO_TIME']<=end)]\n",
    "len(year20['SID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码后维度 (38636, 42)\n",
      "42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SID</th>\n",
       "      <th>ISO_TIME</th>\n",
       "      <th>LON</th>\n",
       "      <th>LAT</th>\n",
       "      <th>STORM_SPEED</th>\n",
       "      <th>DIST2LAND</th>\n",
       "      <th>STORM_DIR</th>\n",
       "      <th>LANDFALL</th>\n",
       "      <th>USA_WIND</th>\n",
       "      <th>TOKYO_R30_DIR</th>\n",
       "      <th>TOKYO_GRADE</th>\n",
       "      <th>TOKYO_R50_DIR</th>\n",
       "      <th>TOKYO_PRES</th>\n",
       "      <th>USA_POCI</th>\n",
       "      <th>USA_ROCI</th>\n",
       "      <th>USA_RMW</th>\n",
       "      <th>TOKYO_R30_LONG</th>\n",
       "      <th>TOKYO_R30_SHORT</th>\n",
       "      <th>USA_R34_NE</th>\n",
       "      <th>USA_R34_SE</th>\n",
       "      <th>USA_R34_NW</th>\n",
       "      <th>USA_R34_SW</th>\n",
       "      <th>TOKYO_LAND</th>\n",
       "      <th>TOKYO_R50_LONG</th>\n",
       "      <th>TOKYO_R50_SHORT</th>\n",
       "      <th>USA_R50_NE</th>\n",
       "      <th>USA_R50_SE</th>\n",
       "      <th>USA_R50_NW</th>\n",
       "      <th>USA_R50_SW</th>\n",
       "      <th>NATURE_DS</th>\n",
       "      <th>NATURE_ET</th>\n",
       "      <th>NATURE_MX</th>\n",
       "      <th>NATURE_NR</th>\n",
       "      <th>NATURE_SS</th>\n",
       "      <th>NATURE_TS</th>\n",
       "      <th>SUBBASIN_MM</th>\n",
       "      <th>BASIN_WP</th>\n",
       "      <th>TRACK_TYPE_PROVISIONAL</th>\n",
       "      <th>TRACK_TYPE_main</th>\n",
       "      <th>TRACK_TYPE_spur-merge</th>\n",
       "      <th>TRACK_TYPE_spur-other</th>\n",
       "      <th>TRACK_TYPE_spur-split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000125N06136</td>\n",
       "      <td>2000-05-03 18:00:00</td>\n",
       "      <td>0.446468</td>\n",
       "      <td>0.0910569</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.243235</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.243541</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000125N06136</td>\n",
       "      <td>2000-05-03 21:00:00</td>\n",
       "      <td>0.444404</td>\n",
       "      <td>0.100366</td>\n",
       "      <td>0.126437</td>\n",
       "      <td>0.264317</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.26465</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000125N06136</td>\n",
       "      <td>2000-05-04 00:00:00</td>\n",
       "      <td>0.442716</td>\n",
       "      <td>0.108943</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.281938</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.282294</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000125N06136</td>\n",
       "      <td>2000-05-04 03:00:00</td>\n",
       "      <td>0.441715</td>\n",
       "      <td>0.116179</td>\n",
       "      <td>0.091954</td>\n",
       "      <td>0.29421</td>\n",
       "      <td>0.980556</td>\n",
       "      <td>0.294581</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000125N06136</td>\n",
       "      <td>2000-05-04 06:00:00</td>\n",
       "      <td>0.441465</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.0689655</td>\n",
       "      <td>0.302077</td>\n",
       "      <td>0.00555556</td>\n",
       "      <td>0.302457</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             SID             ISO_TIME       LON        LAT STORM_SPEED  \\\n",
       "0  2000125N06136  2000-05-03 18:00:00  0.446468  0.0910569    0.137931   \n",
       "1  2000125N06136  2000-05-03 21:00:00  0.444404   0.100366    0.126437   \n",
       "2  2000125N06136  2000-05-04 00:00:00  0.442716   0.108943    0.114943   \n",
       "3  2000125N06136  2000-05-04 03:00:00  0.441715   0.116179    0.091954   \n",
       "4  2000125N06136  2000-05-04 06:00:00  0.441465   0.121951   0.0689655   \n",
       "\n",
       "  DIST2LAND   STORM_DIR  LANDFALL USA_WIND TOKYO_R30_DIR TOKYO_GRADE  \\\n",
       "0  0.243235    0.955556  0.243541  0.09375           NaN         NaN   \n",
       "1  0.264317    0.958333   0.26465  0.09375           NaN         NaN   \n",
       "2  0.281938    0.966667  0.282294  0.09375           NaN         NaN   \n",
       "3   0.29421    0.980556  0.294581  0.09375           NaN         NaN   \n",
       "4  0.302077  0.00555556  0.302457  0.09375           NaN         NaN   \n",
       "\n",
       "  TOKYO_R50_DIR TOKYO_PRES USA_POCI USA_ROCI USA_RMW TOKYO_R30_LONG  \\\n",
       "0           NaN        NaN      NaN      NaN     NaN            NaN   \n",
       "1           NaN        NaN      NaN      NaN     NaN            NaN   \n",
       "2           NaN        NaN      NaN      NaN     NaN            NaN   \n",
       "3           NaN        NaN      NaN      NaN     NaN            NaN   \n",
       "4           NaN        NaN      NaN      NaN     NaN            NaN   \n",
       "\n",
       "  TOKYO_R30_SHORT USA_R34_NE USA_R34_SE USA_R34_NW USA_R34_SW TOKYO_LAND  \\\n",
       "0             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "  TOKYO_R50_LONG TOKYO_R50_SHORT USA_R50_NE USA_R50_SE USA_R50_NW USA_R50_SW  \\\n",
       "0            NaN             NaN        NaN        NaN        NaN        NaN   \n",
       "1            NaN             NaN        NaN        NaN        NaN        NaN   \n",
       "2            NaN             NaN        NaN        NaN        NaN        NaN   \n",
       "3            NaN             NaN        NaN        NaN        NaN        NaN   \n",
       "4            NaN             NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "  NATURE_DS NATURE_ET NATURE_MX NATURE_NR NATURE_SS NATURE_TS SUBBASIN_MM  \\\n",
       "0         0         0         0         0         0         1           0   \n",
       "1         0         0         0         0         0         1           0   \n",
       "2         0         0         0         0         0         1           0   \n",
       "3         0         0         0         0         0         1           0   \n",
       "4         0         0         0         0         0         1           0   \n",
       "\n",
       "  BASIN_WP TRACK_TYPE_PROVISIONAL TRACK_TYPE_main TRACK_TYPE_spur-merge  \\\n",
       "0        0                      0               1                     0   \n",
       "1        0                      0               1                     0   \n",
       "2        0                      0               1                     0   \n",
       "3        0                      0               1                     0   \n",
       "4        0                      0               1                     0   \n",
       "\n",
       "  TRACK_TYPE_spur-other TRACK_TYPE_spur-split  \n",
       "0                     0                     0  \n",
       "1                     0                     0  \n",
       "2                     0                     0  \n",
       "3                     0                     0  \n",
       "4                     0                     0  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据预处理\n",
    "# one-hot 编码\n",
    "# 归一化特征 TRACK_TYPE, NATURE,SUBBASIN,BASIN,HKO_CAT\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    " \n",
    "class One_hot_encoder:\n",
    "    def __init__(self, df, column_name_list):\n",
    "        self.df = df\n",
    "        self.column_name_list = column_name_list\n",
    "\n",
    "    def multi_column_encoder(self):\n",
    "        Enc_ohe, Enc_label = OneHotEncoder(), LabelEncoder()\n",
    "        for column_name in self.column_name_list:\n",
    "            self.df[\"Dummies\"] = Enc_label.fit_transform(self.df[column_name])\n",
    "            self.df_dummies = pd.DataFrame(Enc_ohe.fit_transform(self.df[[\"Dummies\"]]).todense(),\n",
    "                                           columns=Enc_label.classes_)\n",
    "            self.df_dummies.rename(columns=lambda x: column_name + \"_\" + x, inplace=True)\n",
    "            self.df = pd.concat([self.df, self.df_dummies], axis=1)\n",
    "            self.df.drop([\"Dummies\"], axis=1, inplace=True)\n",
    "        self.df.drop(self.column_name_list, axis=1, inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    \n",
    "column_name_list = ['NATURE', 'SUBBASIN', 'BASIN','TRACK_TYPE']\n",
    "dataNew = pd.read_csv('original_data.csv')\n",
    "dataNew.drop(['DEL'],axis = 1,inplace=True)\n",
    "\n",
    "df_encoded = One_hot_encoder(dataNew, column_name_list).multi_column_encoder()\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 编码后维度\n",
    "print('编码后维度',df_encoded.shape)\n",
    "# df_encoded.head()\n",
    "\n",
    "# # 归一化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "minMaxScaler = MinMaxScaler()\n",
    "data_str = df_encoded[['SID','ISO_TIME']]\n",
    "data_num = df_encoded.iloc[:,2:]\n",
    "num_columns = data_num.columns\n",
    "col = ['SID','ISO_TIME']\n",
    "col += list(num_columns)\n",
    "print(len(col))\n",
    "data_v = data_num.values.astype(np.float)\n",
    "data_v = minMaxScaler.fit_transform(data_v)\n",
    "data_v = np.hstack((data_str.values,data_v))\n",
    "data = pd.DataFrame(data_v,columns=col)\n",
    "data.to_csv('data_norm.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集 epoch:1,loss:0.8006332516670227\n",
      "测试集 epoch:1,loss is 1.1007332801818848\n",
      "训练集 epoch:2,loss:1.0639559030532837\n",
      "测试集 epoch:2,loss is 1.048992395401001\n",
      "训练集 epoch:3,loss:1.0314024686813354\n",
      "测试集 epoch:3,loss is 1.0317575931549072\n",
      "训练集 epoch:4,loss:1.0220624208450317\n",
      "测试集 epoch:4,loss is 1.0317575931549072\n",
      "训练集 epoch:5,loss:0.9319124221801758\n",
      "测试集 epoch:5,loss is 0.9359242916107178\n",
      "训练集 epoch:6,loss:0.9424735307693481\n",
      "测试集 epoch:6,loss is 0.9359242916107178\n",
      "训练集 epoch:7,loss:0.9260362982749939\n",
      "测试集 epoch:7,loss is 0.9359242916107178\n",
      "训练集 epoch:8,loss:0.9420243501663208\n",
      "测试集 epoch:8,loss is 0.9359242916107178\n",
      "训练集 epoch:9,loss:0.9392698407173157\n",
      "测试集 epoch:9,loss is 0.9359242916107178\n",
      "训练集 epoch:10,loss:0.9291229248046875\n",
      "测试集 epoch:10,loss is 0.9359242916107178\n",
      "训练集 epoch:11,loss:0.9313182830810547\n",
      "测试集 epoch:11,loss is 0.9359242916107178\n",
      "训练集 epoch:12,loss:0.9404674172401428\n",
      "测试集 epoch:12,loss is 0.9359242916107178\n",
      "训练集 epoch:13,loss:0.9382112622261047\n",
      "测试集 epoch:13,loss is 0.9359242916107178\n",
      "训练集 epoch:14,loss:0.9243780374526978\n",
      "测试集 epoch:14,loss is 0.9280717372894287\n",
      "训练集 epoch:15,loss:0.9270986914634705\n",
      "测试集 epoch:15,loss is 0.9280717372894287\n",
      "训练集 epoch:16,loss:0.9365124702453613\n",
      "测试集 epoch:16,loss is 0.9280717372894287\n",
      "训练集 epoch:17,loss:0.8965330719947815\n",
      "测试集 epoch:17,loss is 0.917104959487915\n",
      "训练集 epoch:18,loss:0.9432604908943176\n",
      "测试集 epoch:18,loss is 0.9475215673446655\n",
      "训练集 epoch:19,loss:0.8812837600708008\n",
      "测试集 epoch:19,loss is 0.8639411926269531\n",
      "训练集 epoch:20,loss:0.8432735204696655\n",
      "测试集 epoch:20,loss is 0.8536810278892517\n",
      "训练集 epoch:21,loss:0.8650194406509399\n",
      "测试集 epoch:21,loss is 0.8536810278892517\n",
      "训练集 epoch:22,loss:0.8967146873474121\n",
      "测试集 epoch:22,loss is 0.898434579372406\n",
      "训练集 epoch:23,loss:0.03832896053791046\n",
      "测试集 epoch:23,loss is 0.03703099116683006\n",
      "训练集 epoch:24,loss:0.026051947847008705\n",
      "测试集 epoch:24,loss is 0.02591056376695633\n",
      "训练集 epoch:25,loss:0.020878765732049942\n",
      "测试集 epoch:25,loss is 0.02142934687435627\n",
      "训练集 epoch:26,loss:0.019246645271778107\n",
      "测试集 epoch:26,loss is 0.018524928018450737\n",
      "训练集 epoch:27,loss:0.015230954624712467\n",
      "测试集 epoch:27,loss is 0.016493570059537888\n",
      "训练集 epoch:28,loss:0.015251795761287212\n",
      "测试集 epoch:28,loss is 0.015226738527417183\n",
      "训练集 epoch:29,loss:0.015321020968258381\n",
      "测试集 epoch:29,loss is 0.01397461723536253\n",
      "训练集 epoch:30,loss:0.01237022690474987\n",
      "测试集 epoch:30,loss is 0.013019817881286144\n",
      "训练集 epoch:31,loss:0.013863280415534973\n",
      "测试集 epoch:31,loss is 0.012669725343585014\n",
      "训练集 epoch:32,loss:0.012565295211970806\n",
      "测试集 epoch:32,loss is 0.012602177448570728\n",
      "训练集 epoch:33,loss:0.013300305232405663\n",
      "测试集 epoch:33,loss is 0.012417673133313656\n",
      "训练集 epoch:34,loss:0.012623737566173077\n",
      "测试集 epoch:34,loss is 0.012140119448304176\n",
      "训练集 epoch:35,loss:0.01155416015535593\n",
      "测试集 epoch:35,loss is 0.011873027309775352\n",
      "训练集 epoch:36,loss:0.011989759281277657\n",
      "测试集 epoch:36,loss is 0.0117324935272336\n",
      "训练集 epoch:37,loss:0.009315213188529015\n",
      "测试集 epoch:37,loss is 0.011649263091385365\n",
      "训练集 epoch:38,loss:0.012781234458088875\n",
      "测试集 epoch:38,loss is 0.011526050977408886\n",
      "训练集 epoch:39,loss:0.010710055008530617\n",
      "测试集 epoch:39,loss is 0.0112924100831151\n",
      "训练集 epoch:40,loss:0.011271102353930473\n",
      "测试集 epoch:40,loss is 0.010770272463560104\n",
      "训练集 epoch:41,loss:0.0073960162699222565\n",
      "测试集 epoch:41,loss is 0.008562298491597176\n",
      "训练集 epoch:42,loss:0.009816613048315048\n",
      "测试集 epoch:42,loss is 0.008371821604669094\n",
      "训练集 epoch:43,loss:0.006627843715250492\n",
      "测试集 epoch:43,loss is 0.008039312437176704\n",
      "训练集 epoch:44,loss:0.008224163204431534\n",
      "测试集 epoch:44,loss is 0.008099725469946861\n",
      "训练集 epoch:45,loss:0.007428668439388275\n",
      "测试集 epoch:45,loss is 0.008226924575865269\n",
      "训练集 epoch:46,loss:0.006406736560165882\n",
      "测试集 epoch:46,loss is 0.007669295184314251\n",
      "训练集 epoch:47,loss:0.008287183940410614\n",
      "测试集 epoch:47,loss is 0.007567073684185743\n",
      "训练集 epoch:48,loss:0.006099277641624212\n",
      "测试集 epoch:48,loss is 0.007526631932705641\n",
      "训练集 epoch:49,loss:0.00942675769329071\n",
      "测试集 epoch:49,loss is 0.007535032462328672\n",
      "训练集 epoch:50,loss:0.008732743561267853\n",
      "测试集 epoch:50,loss is 0.007493808399885893\n",
      "训练集 epoch:51,loss:0.007592448499053717\n",
      "测试集 epoch:51,loss is 0.007385362405329943\n",
      "训练集 epoch:52,loss:0.006122400052845478\n",
      "测试集 epoch:52,loss is 0.007210991345345974\n",
      "训练集 epoch:53,loss:0.00737194437533617\n",
      "测试集 epoch:53,loss is 0.007112243678420782\n",
      "训练集 epoch:54,loss:0.0075337267480790615\n",
      "测试集 epoch:54,loss is 0.007221153471618891\n",
      "训练集 epoch:55,loss:0.006220777053385973\n",
      "测试集 epoch:55,loss is 0.007208727765828371\n",
      "训练集 epoch:56,loss:0.007319342344999313\n",
      "测试集 epoch:56,loss is 0.0070517342537641525\n",
      "训练集 epoch:57,loss:0.006898987106978893\n",
      "测试集 epoch:57,loss is 0.007035238668322563\n",
      "训练集 epoch:58,loss:0.007501524407416582\n",
      "测试集 epoch:58,loss is 0.006831583101302385\n",
      "训练集 epoch:59,loss:0.007366582751274109\n",
      "测试集 epoch:59,loss is 0.0068163787946105\n",
      "训练集 epoch:60,loss:0.006786462385207415\n",
      "测试集 epoch:60,loss is 0.006776250433176756\n",
      "训练集 epoch:61,loss:0.008047421462833881\n",
      "测试集 epoch:61,loss is 0.006844205781817436\n",
      "训练集 epoch:62,loss:0.006805541459470987\n",
      "测试集 epoch:62,loss is 0.006783320568501949\n",
      "训练集 epoch:63,loss:0.008049488998949528\n",
      "测试集 epoch:63,loss is 0.0068777077831327915\n",
      "训练集 epoch:64,loss:0.007784325163811445\n",
      "测试集 epoch:64,loss is 0.006718715187162161\n",
      "训练集 epoch:65,loss:0.007662530522793531\n",
      "测试集 epoch:65,loss is 0.006763346027582884\n",
      "训练集 epoch:66,loss:0.008265485987067223\n",
      "测试集 epoch:66,loss is 0.006623880472034216\n",
      "训练集 epoch:67,loss:0.005775854457169771\n",
      "测试集 epoch:67,loss is 0.00653157290071249\n",
      "训练集 epoch:68,loss:0.005316551309078932\n",
      "测试集 epoch:68,loss is 0.006656727287918329\n",
      "训练集 epoch:69,loss:0.006677743513137102\n",
      "测试集 epoch:69,loss is 0.006503620184957981\n",
      "训练集 epoch:70,loss:0.007130480837076902\n",
      "测试集 epoch:70,loss is 0.0064781224355101585\n",
      "训练集 epoch:71,loss:0.006826227530837059\n",
      "测试集 epoch:71,loss is 0.006516064051538706\n",
      "训练集 epoch:72,loss:0.007198658771812916\n",
      "测试集 epoch:72,loss is 0.006381487939506769\n",
      "训练集 epoch:73,loss:0.005623991601169109\n",
      "测试集 epoch:73,loss is 0.006524408236145973\n",
      "训练集 epoch:74,loss:0.0063047450967133045\n",
      "测试集 epoch:74,loss is 0.006497442722320557\n",
      "训练集 epoch:75,loss:0.008493109606206417\n",
      "测试集 epoch:75,loss is 0.00620351592078805\n",
      "训练集 epoch:76,loss:0.007103839889168739\n",
      "测试集 epoch:76,loss is 0.006380691658705473\n",
      "训练集 epoch:77,loss:0.006505271419882774\n",
      "测试集 epoch:77,loss is 0.006262178532779217\n",
      "训练集 epoch:78,loss:0.007626312784850597\n",
      "测试集 epoch:78,loss is 0.006155585404485464\n",
      "训练集 epoch:79,loss:0.005837869830429554\n",
      "测试集 epoch:79,loss is 0.0061231860890984535\n",
      "训练集 epoch:80,loss:0.005931233521550894\n",
      "测试集 epoch:80,loss is 0.006054127588868141\n",
      "训练集 epoch:81,loss:0.0055160666815936565\n",
      "测试集 epoch:81,loss is 0.006129117216914892\n",
      "训练集 epoch:82,loss:0.00847644917666912\n",
      "测试集 epoch:82,loss is 0.006024263799190521\n",
      "训练集 epoch:83,loss:0.004969883244484663\n",
      "测试集 epoch:83,loss is 0.005974065978080034\n",
      "训练集 epoch:84,loss:0.006745567545294762\n",
      "测试集 epoch:84,loss is 0.006067981477826834\n",
      "训练集 epoch:85,loss:0.005860896315425634\n",
      "测试集 epoch:85,loss is 0.005896749906241894\n",
      "训练集 epoch:86,loss:0.007810269948095083\n",
      "测试集 epoch:86,loss is 0.0058129820972681046\n",
      "训练集 epoch:87,loss:0.00839400477707386\n",
      "测试集 epoch:87,loss is 0.006059880368411541\n",
      "训练集 epoch:88,loss:0.005827202927321196\n",
      "测试集 epoch:88,loss is 0.005857415031641722\n",
      "训练集 epoch:89,loss:0.007232001516968012\n",
      "测试集 epoch:89,loss is 0.005738017614930868\n",
      "训练集 epoch:90,loss:0.0053593371994793415\n",
      "测试集 epoch:90,loss is 0.005781994201242924\n",
      "训练集 epoch:91,loss:0.005826447159051895\n",
      "测试集 epoch:91,loss is 0.005894636735320091\n",
      "训练集 epoch:92,loss:0.00721357436850667\n",
      "测试集 epoch:92,loss is 0.005915236193686724\n",
      "训练集 epoch:93,loss:0.007804393768310547\n",
      "测试集 epoch:93,loss is 0.006026362534612417\n",
      "训练集 epoch:94,loss:0.004114271141588688\n",
      "测试集 epoch:94,loss is 0.005868119187653065\n",
      "训练集 epoch:95,loss:0.006821108050644398\n",
      "测试集 epoch:95,loss is 0.006093816831707954\n",
      "训练集 epoch:96,loss:0.005547348875552416\n",
      "测试集 epoch:96,loss is 0.005830684211105108\n",
      "训练集 epoch:97,loss:0.00706658698618412\n",
      "测试集 epoch:97,loss is 0.005826183129101992\n",
      "训练集 epoch:98,loss:0.0058189574629068375\n",
      "测试集 epoch:98,loss is 0.005777608137577772\n",
      "训练集 epoch:99,loss:0.006487537641078234\n",
      "测试集 epoch:99,loss is 0.0058098239824175835\n",
      "训练集 epoch:100,loss:0.008505789563059807\n",
      "测试集 epoch:100,loss is 0.005909086670726538\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.70041608810425\n"
     ]
    }
   ],
   "source": [
    "# 训练啦\n",
    "import torch\n",
    "import time\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt# 数据加载\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "class DataSet(Dataset):\n",
    "    '''\n",
    "    构建数据集\n",
    "    '''\n",
    "    def __init__(self,data_pd):\n",
    "        data_DAE = np.array(data_pd)\n",
    "        self.x_data = torch.from_numpy(data_DAE[:,:]).type(torch.float32)\n",
    "        self.y_data = torch.from_numpy(data_DAE[:,:]).type(torch.float32)\n",
    "        self.len = data_DAE.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index],self.y_data[index]\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        返回数据的数目\n",
    "        '''\n",
    "        return self.len\n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1:\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# 训练模型啦！\n",
    "class Auto_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Auto_Encoder,self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Dropout(p=0.5),\n",
    "                                     nn.Linear(40,32),\n",
    "                                    nn.ReLU(True),\n",
    "                                     nn.Dropout(p=0.5),\n",
    "                                    nn.Linear(32,16),\n",
    "                                    nn.ReLU(True),\n",
    "#                                     nn.Dropout(p=0.5),\n",
    "#                                     nn.Linear(24,16),\n",
    "#                                     nn.ReLU(True),\n",
    "                                     \n",
    "                                     nn.Dropout(p=0.5),\n",
    "                                      nn.Linear(16,8),\n",
    "                                    nn.ReLU(True),\n",
    "                                     nn.Dropout(p=0.5),\n",
    "                                     nn.Linear(8,4)\n",
    "                                     \n",
    "                                    )\n",
    "        self.decoder = nn.Sequential( nn.Dropout(p=0.5),\n",
    "                                     nn.Linear(4,8),\n",
    "                                    nn.ReLU(True),\n",
    "                                     nn.Dropout(p=0.5),\n",
    "                                    nn.Linear(8,16),\n",
    "                                    nn.ReLU(True),\n",
    "#                                      nn.Dropout(p=0.5),\n",
    "#                                      nn.Linear(16,24),\n",
    "#                                      nn.ReLU(True),\n",
    "                                     nn.Dropout(p=0.5),\n",
    "                                     nn.Linear(16,32),\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.Dropout(p=0.5),\n",
    "                                     nn.Linear(32,40),\n",
    "                                     nn.Tanh()\n",
    "#                                      nn.Sigmoid()\n",
    "                                    )\n",
    "    def forward(self,x):\n",
    "        encoder = self.encoder(x)\n",
    "        decoder = self.decoder(encoder)\n",
    "        return encoder,decoder\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 画图用曲线\n",
    "    start = time.time()\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    x_epoch = []\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "    # 表示L2正则化\n",
    "    weight_decay = 1e-4\n",
    "    epoches = 100\n",
    "    model = Auto_Encoder()\n",
    "#     model.apply(weights_init)\n",
    "\n",
    "# 权重初始化\n",
    "    for m in model.modules():\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            nn.init.normal(m.weight.data)\n",
    "            nn.init.xavier_normal(m.weight.data)\n",
    "            nn.init.kaiming_normal(m.weight.data)#卷积层参数初始化\n",
    "            m.bias.data.fill_(0)\n",
    "        elif isinstance(m,nn.Linear):\n",
    "            m.weight.data.normal_()#全连接层参数初始化\n",
    "            \n",
    "    data_all = pd.read_csv('data_norm.csv')\n",
    "    data_all = data_all.replace({np.nan:0.0})\n",
    "\n",
    "#     data_all.values.replaces({np.nan:'0'})\n",
    "    # 去掉ID和时间以及序号\n",
    "    data_all = data_all.iloc[:,3:]\n",
    "    # 数据集划分 \n",
    "    data_1,data_2= train_test_split(data_all,test_size=0.2,shuffle=True)\n",
    "    \n",
    "    train_data = DataSet(data_1)\n",
    "    test_data = DataSet(data_2)\n",
    "    # dataLoader构造\n",
    "    train_loader = DataLoader(train_data,batch_size = batch_size,shuffle = True)\n",
    "    test_loader = DataLoader(test_data,batch_size=batch_size,shuffle = False)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizier = optim.Adam(model.parameters(),lr = lr,weight_decay = weight_decay)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda\n",
    "    for epoch in range(epoches):\n",
    "        train_batch_total = 0\n",
    "        test_batch_total = 0\n",
    "        test_loss = 0\n",
    "        train_loss = 0        \n",
    "#         for param_group in optimizier.param_groups:\n",
    "#             param_group['lr'] *= 0.1\n",
    "        # 训练\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            x,y = data\n",
    "            optimizier.zero_grad()\n",
    "            hidden_vector,output = model(x)\n",
    "            loss = criterion(output,y)\n",
    "            loss.backward()\n",
    "            optimizier.step()\n",
    "        print('训练集 epoch:{},loss:{}'.format(epoch+1,loss.item()))\n",
    "        loss_train.append(loss.item())\n",
    "        \n",
    "        # 测试\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx,data in enumerate(test_loader):\n",
    "                x,y = data\n",
    "                hidden_vector,output = model(x)\n",
    "                loss = criterion(output,y)\n",
    "#                 test_loss += loss.item()\n",
    "                test_batch_total += 1\n",
    "#             print('测试集 epoch:{},loss is {}'.format((epoch+1),test_loss/test_batch_total))\n",
    "            print('测试集 epoch:{},loss is {}'.format((epoch+1),loss.item()))\n",
    "            loss_test.append(loss.item())\n",
    "        \n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), 'AE_model.pkl')  \n",
    " \n",
    "    \n",
    "#     torch.save(model, 'AE_model.pkl')\n",
    "# 可视化\n",
    "    x_epoch = np.arange(1,101)\n",
    "    plt.figure()\n",
    "    plt.plot(x_epoch,loss_train,label='train_loss',marker='+',linewidth=1)\n",
    "    plt.plot(x_epoch,loss_test,label='test_loss',marker='^',linewidth=1)\n",
    "    plt.xlabel('Epoches')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    end = time.time()\n",
    "    print(end-start)  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-55bcafa03ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAuto_Encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# 获得DAE的数据\n",
    "import torch\n",
    "# import Auto_Encoder\n",
    "model_path = 'AE_model.pkl'\n",
    "\n",
    "model = Auto_Encoder()  \n",
    "model.load_state_dict(torch.load(model_path))  \n",
    "\n",
    "\n",
    "# model = torch.load(model_path)\n",
    "\n",
    "norm_origial = pd.read_csv('data_norm.csv')\n",
    "norm_origial.drop(['I'],axis = 1,inplace=True)\n",
    "\n",
    "data = norm_origial.iloc[:,2:]\n",
    "data_id = norm_origial.iloc[:,:2]\n",
    "label = norm_origial[['LAT','LON']]\n",
    "\n",
    "data_all = data.replace({np.nan:0.0})\n",
    "data_DAE = np.array(data_all)\n",
    "x_data = torch.from_numpy(data_DAE[:,:]).type(torch.float32)\n",
    "hidden_vector_torch,_ = model(x_data)\n",
    "\n",
    "hidden_vector = pd.DataFrame(hidden_vector_torch.detach().numpy(),columns=['h1','h2','h3','h4'])\n",
    "cols = ['SID','TIME','h1','h2','h3','h4','LAT','LON']\n",
    "# print(hidden_vector)\n",
    "\n",
    "hidden_vector_value = np.hstack((data_id.values,hidden_vector))\n",
    "hidden_vector_value = np.hstack((hidden_vector_value,label.values))\n",
    "hidden_vector_data = pd.DataFrame(hidden_vector_value,columns=cols)\n",
    "\n",
    "# print(hidden_vector_data.head())\n",
    "\n",
    "hidden_vector_data.to_csv('hidden_vector_data.csv')\n",
    "\n",
    "# 监督化\n",
    "\n",
    "def series_to_supervised(data,n_in,n_out,Label):\n",
    "#     label : list 标签\n",
    "    col_name = data.columns    \n",
    "    cols,names = list(),list()\n",
    "    # 输入序列\n",
    "    for i in range(n_in,0,-1):\n",
    "        cols.append(data.shift(i))\n",
    "        names += [(x+'(t-%d)')%i for x in col_name]\n",
    "       \n",
    "    # 预测序列 (t,t+1,...,t+n)\n",
    "    for i in range(0,n_out):\n",
    "        for j in Label:\n",
    "            cols.append(data[j].shift(-i))  \n",
    "            names += ['label_'+j+'_(t+%d)'%i]    \n",
    "#     print(names)\n",
    "    data_super = pd.concat(cols,axis = 1)\n",
    "    data_super.columns = names\n",
    "    \n",
    "    return data_super,cols\n",
    "\n",
    "def supervised(data):\n",
    "#     k:总步长\n",
    "    start = 0\n",
    "    data_new = pd.DataFrame()\n",
    "    count = 0\n",
    "    while start < len(data) - 2:\n",
    "        target = start + 1\n",
    "        # start 定死了，target 往后移\n",
    "        while data['SID'][start] == data['SID'][target] and target < len(data) - 1:\n",
    "            target += 1\n",
    "        count += 1\n",
    "#         print('start:%d,end:%d'%(start,target))\n",
    "        data_local, cols = series_to_supervised(data[start:target],16,8,['LAT','LON']) \n",
    "        data_new = data_new.append(data_local,ignore_index=True)\n",
    "        start = target\n",
    "    return data_new,count  \n",
    "\n",
    "data_new,count = supervised(hidden_vector_data)\n",
    "del_features_1 = [['SID(t-'+str(i)+')','TIME(t-'+str(i)+')'] for i in range(16,0,-1)]\n",
    "\n",
    "for i in del_features_1:\n",
    "    data_new.drop(i,axis = 1,inplace=True)\n",
    "\n",
    "data_new.to_csv('supervised_data.csv')\n",
    "# print(data_new.head())\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_hid_dim 512\n",
      "dec_hid_dim 512\n",
      "模型结构 Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(6, 64, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (attention): Attention(\n",
      "    (attn): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "  )\n",
      "  (decoder): AttentionDecoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
      "    )\n",
      "    (rnn1): LSTM(512, 64, num_layers=3, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=128, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "x shape torch.Size([1, 96])\n",
      "y shape torch.Size([1, 16])\n",
      "初始xshape： torch.Size([1, 96])\n",
      "x reshape： torch.Size([1, 16, 6])\n",
      "Encoder 结束\n",
      "x:torch.Size([1, 16, 64]),hidden:torch.Size([3, 1, 64]),cell:torch.Size([3, 1, 64])\n",
      "Attention 结束\n",
      "tensor([[0.0624, 0.0625, 0.0626, 0.0626, 0.0625, 0.0624, 0.0624, 0.0626, 0.0625,\n",
      "         0.0625, 0.0624, 0.0626, 0.0626, 0.0626, 0.0625, 0.0624]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "rnn_input shape torch.Size([1, 1, 65])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 512, got 65",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-446e8a074863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y shape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0moptimizier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;31m#         output = model(seq_inp,seq_inp[seq_length-1:seq_length,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/study/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-446e8a074863>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prev_y)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m#itearate over LSTM - according to the required output days\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mout_days\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mprev_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mprev_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/study/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-446e8a074863>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, input_hidden, input_cell, encoder_outputs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rnn_input shape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_n\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mweighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/study/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/study/venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/PycharmProjects/study/venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/study/venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    175\u001b[0m                     expected_input_dim, input.dim()))\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    178\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m    179\u001b[0m                     self.input_size, input.size(-1)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 512, got 65"
     ]
    }
   ],
   "source": [
    "# Seq2Seq_Atten\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt# 数据加载\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 专属dataset\n",
    "class DataSet_seq(Dataset):\n",
    "    '''\n",
    "    构建数据集\n",
    "    '''\n",
    "    def __init__(self,data_pd):\n",
    "        data = np.array(data_pd)\n",
    "        self.x_data = torch.from_numpy(data[:,0:16*6]).type(torch.float32)\n",
    "        self.y_data = torch.from_numpy(data[:,16*6:]).type(torch.float32)\n",
    "        self.len = data.shape[0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index],self.y_data[index]\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        返回数据的数目\n",
    "        '''\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,seq_len,n_features,embedding_dim = 64):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.seq_len,self.n_features = seq_len,n_features\n",
    "        self.embedding_dim,self.hidden_dim = embedding_dim,embedding_dim\n",
    "        self.num_layers = 3\n",
    "        self.rnn1 = nn.LSTM(\n",
    "        input_size = n_features,\n",
    "        hidden_size = self.hidden_dim,\n",
    "        num_layers = 3,\n",
    "        batch_first = True,\n",
    "        dropout = 0.35\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        print('初始xshape：',x.shape)\n",
    "        x = x.reshape((batch_sizes,self.seq_len,self.n_features))\n",
    "        print('x reshape：',x.shape)\n",
    "        h_1 = torch.zeros(self.num_layers,x.size(0),self.hidden_dim)\n",
    "        c_1 = torch.zeros(self.num_layers,x.size(0),self.hidden_dim)\n",
    "        x,(hidden,cell) = self.rnn1(x,(h_1,c_1,))\n",
    "        print('Encoder 结束')\n",
    "        print('x:{},hidden:{},cell:{}'.format(x.shape,hidden.shape,cell.shape))\n",
    "        return x,hidden,cell\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,enc_hid_dim,dec_hid_dim):\n",
    "        super().__init__()\n",
    "        print('enc_hid_dim',enc_hid_dim)\n",
    "        print('dec_hid_dim',dec_hid_dim)\n",
    "#         self.attn = nn.Linear((enc_hid_dim)+dec_hid_dim,dec_hid_dim) \n",
    "        self.attn = nn.Linear(128,dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim,1,bias = False)\n",
    "    def forward(self,hidden,encoder_outputs):\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden[2:3,:,:]\n",
    "        hidden = hidden.repeat(1,src_len,1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden,encoder_outputs),dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        print('Attention 结束')\n",
    "        print(F.softmax(attention,dim=1))\n",
    "        return F.softmax(attention,dim=1)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,seq_len,input_dim=64,n_features=2):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.seq_len,self.input_dim = seq_len,input_dim\n",
    "        self.hidden_dim,self.n_features = input_dim,n_features\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=input_dim,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            dropout = 0.35\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.hidden_dim,n_features)\n",
    "    def forward(self, x,input_hidden,input_cell): \n",
    "        print('decoder x shape:',x.shape)\n",
    "        print(x.shape)\n",
    "        x = x.reshape((1,1,1))\n",
    "        x, (hidden_n, cell_n) = self.rnn1(x,(input_hidden,input_cell))\n",
    "        x = self.output_layer(x)\n",
    "        print('Decoder结束')\n",
    "        print('x:{},hidden_n:{},cell_n:{}'.format(x.shape,hidden_n.shape,cell_n.shape))\n",
    "        return x, hidden_n, cell_n\n",
    "    \n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self,seq_len,attention,input_dim = 64,n_features=2,encoder_hidden_state=512):\n",
    "        super(AttentionDecoder,self).__init__()\n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "        self.hidden_dim, self.n_features =  input_dim, n_features\n",
    "        self.attention = attention \n",
    "        self.rnn1 = nn.LSTM(\n",
    "        input_size = encoder_hidden_state,\n",
    "        hidden_size = input_dim,\n",
    "        num_layers = 3,\n",
    "        dropout = 0.35         \n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.hidden_dim * 2 , n_features)\n",
    "    def forward(self, x,input_hidden,input_cell,encoder_outputs):\n",
    "        a = self.attention(input_hidden, encoder_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        x = x.reshape((1,1,1))\n",
    "        rnn_input = torch.cat((x, weighted), dim = 2)\n",
    "#         print('rnn_input shape',rnn_input.shape)\n",
    "        # rnn_input [1,1,65]\n",
    "        x, (hidden_n, cell_n) = self.rnn1(rnn_input,(input_hidden,input_cell))\n",
    "        output = x.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        x = self.output_layer(torch.cat((output, weighted), dim = 1))\n",
    "        return x, hidden_n, cell_n\n",
    "       \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,seq_len,n_features,embedding_dim = 64,output_length = 8):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = Encoder(seq_len,n_features,embedding_dim)\n",
    "        self.attention = Attention(512,512)\n",
    "        self.output_length = output_length\n",
    "        self.decoder = AttentionDecoder(seq_len,self.attention,embedding_dim,n_features)\n",
    "    def forward(self,x, prev_y):\n",
    "        encoder_output,hidden,cell = self.encoder(x)\n",
    "        #Prepare place holder for decoder output\n",
    "        targets_ta = []\n",
    "        #prev_output become the next input to the LSTM cell\n",
    "        prev_output = prev_y\n",
    "        #itearate over LSTM - according to the required output days\n",
    "        for out_days in range(self.output_length) :\n",
    "            prev_x,prev_hidden,prev_cell = self.decoder(prev_output,hidden,cell,encoder_output)\n",
    "            hidden,cell = prev_hidden,prev_cell\n",
    "            prev_output = prev_x\n",
    "            targets_ta.append(prev_x.reshape(1))\n",
    "        targets = torch.stack(targets_ta)\n",
    "        return targets\n",
    "\n",
    "data = pd.read_csv('supervised_data.csv')\n",
    "\n",
    "# 超参数设置\n",
    "lr = 1e-4\n",
    "epoches = 100\n",
    "criterion = nn.MSELoss()\n",
    "weight_decays = 1e-3\n",
    "batch_sizes = 1\n",
    "\n",
    "# 整体数据\n",
    "data_new = data.iloc[16:,1:]\n",
    "\n",
    "# 准备数据\n",
    "data_train,data_test = train_test_split(data_new,test_size=0.2,shuffle=True) \n",
    "train_data = DataSet_seq(data_train)\n",
    "\n",
    "test_data = DataSet_seq(data_test)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data,batch_size=batch_sizes,shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data,batch_size=batch_sizes,shuffle=False)\n",
    "\n",
    "# 权重初始化\n",
    "def init_weights(m):\n",
    "    for name,parm in m.named_parameters():\n",
    "#         print('参数')\n",
    "#         print(name,parm)\n",
    "        nn.init.uniform_(parm.data,-0.08,0.08)\n",
    "        \n",
    "# 参数设置\n",
    "n_features = 6 # 时刻的特征 6\n",
    "seq_length = 16 # 输入步长\n",
    "label_length = 8 # 输出步长\n",
    "model = Seq2Seq(seq_length,n_features)\n",
    "# init_weights(model)\n",
    "print('模型结构',model)\n",
    "model.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decays)\n",
    "\n",
    "# 训练\n",
    "for epoch in range(epoches):\n",
    "    model = model.train()\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        x,y = data\n",
    "        print('x shape',x.shape)\n",
    "        print('y shape',y.shape)\n",
    "        optimizier.zero_grad()\n",
    "        output = model(x,torch.tensor([[0]]))\n",
    "        \n",
    "#         output = model(seq_inp,seq_inp[seq_length-1:seq_length,:])\n",
    "        \n",
    "        loss = criterion(output,y)\n",
    "        loss.backward()\n",
    "        optimizier.step()\n",
    "    print('训练集 epoch:{},loss:{}'.format(epoch+1,loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8571]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[-0.8571]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172464"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = pd.read_csv('dataset/24_predicted_24_supervised_data.csv')\n",
    "dataNew = data.iloc[:,1:]\n",
    "# 计算data每一行有多少个缺失值的值，即按行统计缺失值\n",
    "rows_null = dataNew.isnull().sum(axis=1) \n",
    "\n",
    "# 下面则是按列统计缺失值\n",
    "col_null = dataNew.isnull().sum(axis=0)\n",
    "\n",
    "#统计整个df的缺失值\n",
    "all_null = dataNew.isnull().sum().sum()\n",
    "\n",
    "all_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
